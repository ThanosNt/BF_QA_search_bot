{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import uuid\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio for running in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize Qdrant clientF\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "# Load SentenceTransformer model\n",
    "MODEL = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Load data and create collection\n",
    "df = pd.read_csv('output_embeddings.csv')\n",
    "text_column_name = 'text'\n",
    "\n",
    "# Recreate collection in Qdrant\n",
    "client.recreate_collection(\n",
    "    collection_name=\"similar_text\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Insert data into Qdrant\n",
    "for index, row in df.iterrows():\n",
    "    text = row[text_column_name]\n",
    "    text_embeddings = MODEL.encode(text).tolist()\n",
    "    id = str(uuid.uuid4())\n",
    "    payload = {\"text\": text, \"text_embeddings\": text_embeddings}\n",
    "    client.upsert(\n",
    "        collection_name=\"similar_text\",\n",
    "        wait=True,\n",
    "        points=[PointStruct(id=id, vector=text_embeddings, payload=payload)]\n",
    "    )\n",
    "\n",
    "# Define the input schema for similarity search\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/search\")\n",
    "def search_similar_text(query: Query):\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=3\n",
    "    )\n",
    "    results = [\n",
    "        {\"text\": result.payload[\"text\"], \"similarity_score\": result.score}\n",
    "        for result in search_result\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dd757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b55262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "# Initialize GPT-Neo model (same setup as before)\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,          # Restrict response length\n",
    "    temperature=0.2,             # Lower randomness for concise answers\n",
    "    repetition_penalty=1.5,      # Increase penalty to reduce repetition\n",
    ")\n",
    "huggingface_llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Context: {context} \"\n",
    "        \"Question: {question} Answer:\"\n",
    "    ),\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=huggingface_llm)\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    # Get context from Qdrant to focus the answer\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=1 # the Limit to most relevant context item\n",
    "    )\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    answer = llm_chain.run(context=context, question=query.question)\n",
    "     \n",
    "    # Clean up answer formatting for readability\n",
    "    answer = answer.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "    # Return JSON response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709046b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import JSONResponse\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "# Initialize GPT-Neo model as before\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "huggingface_llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Set up ChatPromptTemplate\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an assistant for question-answering tasks.\"\n",
    "        \"Based on the context provided, generate a concise answer in three sentences maximum. \"\n",
    "        \"When the generated answer is finished, end it without repeating anything. \"\n",
    "        \"Context: {context} \"\n",
    "        \"Question: {question} Answer:\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message])\n",
    "\n",
    "llm_chain = LLMChain(prompt=chat_prompt, llm=huggingface_llm)\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    # Retrieve context from Qdrant\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Generate answer with ChatPromptTemplate\n",
    "    answer = llm_chain.run(context=context, question=query.question)\n",
    "\n",
    "    # Clean up formatting to truncate any repetitions\n",
    "    answer = answer.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "    # Return JSON response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6555ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize DistilBERT QA model\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Set up the QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search for context from Qdrant (mock example)\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    # Combine context from search results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Get the answer from the QA pipeline\n",
    "    qa_input = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    answer = qa_pipeline(qa_input)\n",
    "    \n",
    "    # Prepare the response with the answer text\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer['answer']\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c024d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f136e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize RoBERTa QA model\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Set up the QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_answer_len=100\n",
    ")\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search for context from Qdrant (mock example)\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    # Combine context from search results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Get the answer from the QA pipeline\n",
    "    qa_input = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    answer = qa_pipeline(qa_input)\n",
    "    \n",
    "    # Prepare the response with the answer text\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer['answer']\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf6441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dadaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize GPT-2 model\n",
    "model_name = \"gpt2-medium\"  # You could also try \"gpt2-medium\" for slightly better performance\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Set up text generation pipeline with tuned parameters for conciseness\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,            # Keep answer length concise\n",
    "    temperature=0.3,               # Control randomness\n",
    "    repetition_penalty=1.3         # Penalize repetition\n",
    ")\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search for context from Qdrant (mock example)\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    # Combine context from search results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Create a prompt combining context and question\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query.question}\\nAnswer:\"\n",
    "\n",
    "    # Generate answer using GPT-2\n",
    "    answer = generator(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # Post-process answer: Cut off after generating two \"Answer:\" statements if present\n",
    "    answer_split = answer.split(\"Answer:\")\n",
    "    if len(answer_split) > 2:\n",
    "        answer = \"Answer:\" + answer_split[1].strip()\n",
    "\n",
    "    # Prepare the response with the answer text\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f554786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize T5 model\n",
    "model_name = \"t5-base\"  # Alternatively, you can use \"t5-base\" or larger variants\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up a text generation pipeline with T5's seq2seq structure for QA\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100,               # Keeps answers concise\n",
    "    repetition_penalty=1.2,      # Helps reduce repetitive answers\n",
    ")\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search for context from Qdrant (mock example)\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    # Combine context from search results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Create a T5-style prompt combining context and question\n",
    "    prompt = f\"question: {query.question} context: {context}\"\n",
    "\n",
    "    # Generate the answer with T5\n",
    "    answer = generator(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # Prepare the response with the answer text\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724e428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize MPT-7B-Chat model\n",
    "model_name = \"mosaicml/mpt-7b-chat\" #too heavy to use for this case and requirements #NOTE\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up a text generation pipeline for question answering\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,         # Keep answers concise\n",
    "    do_sample=True, \n",
    "    repetition_penalty=1.2,     # Reduces repetitive answers\n",
    ")\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search for context from Qdrant\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    # Combine context from search results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Formulate the input for MPT-7B-Chat\n",
    "    prompt = f\"Based on the context provided, answer concisely: {context} Question: {query.question} Answer:\"\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generator(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    # Process answer to remove redundancies\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abac6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize the generative model - DistilGPT-2\n",
    "model_name = \"distilgpt2\" #changed from distilgpt2 to distilbert/distilgpt2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "\n",
    "# Initialize the embedding model for Qdrant (with 768-dimensional embeddings)\n",
    "embedding_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Step 1: Embed the question and perform similarity search in Qdrant\n",
    "    question_embeddings = embedding_model.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2  # Adjust to get the top 2 relevant results\n",
    "    )\n",
    "    \n",
    "    # Step 2: Prepare the context from search results\n",
    "    context = \" \".join([result.payload[\"text\"] for result in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Step 3: Generate the answer using the generative model\n",
    "    input_text = f\"Context: {context} Question: {query.question} Answer:\"\n",
    "    generated_text = generator(input_text, num_return_sequences=1)\n",
    "    \n",
    "    # Step 4: Extract and clean up the generated answer\n",
    "    answer = generated_text[0][\"generated_text\"].replace(input_text, \"\").strip()\n",
    "\n",
    "    # Return JSON response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef3ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set up the sentence-transformers model for creating question embeddings\n",
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    question_embedding = embedding_model.encode(query.question).tolist()\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embedding,\n",
    "        limit=2  # Adjust based on how much context you want\n",
    "    )\n",
    "    \n",
    "    # Step 3: Prepare the context from search results\n",
    "    context = \" \".join([result.payload[\"text\"] for result in search_result]) if search_result else \"\"\n",
    "    \n",
    "    # Step 4: Generate the answer using FLAN-T5\n",
    "    input_text = f\"Context: {context} Question: {query.question} Answer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Return JSON response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d3790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e591314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the Meta-Llama model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# Load the embedding model for Qdrant\n",
    "embedding_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Step 1: Encode the question and search in Qdrant\n",
    "    question_embeddings = embedding_model.encode(query.question).tolist()\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=2\n",
    "    )\n",
    "    \n",
    "    # Step 2: Combine the top contexts\n",
    "    context = \" \".join([result.payload[\"text\"] for result in search_results]) if search_results else \"No relevant context found.\"\n",
    "    \n",
    "    # Step 3: Generate text using the Meta-Llama model\n",
    "    input_prompt = f\"Context: {context} Question: {query.question} Answer:\"\n",
    "    generated_text = generator(input_prompt, num_return_sequences=1)\n",
    "    \n",
    "    # Step 4: Clean and prepare the response\n",
    "    answer = generated_text[0][\"generated_text\"].replace(input_prompt, \"\").strip()\n",
    "    \n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

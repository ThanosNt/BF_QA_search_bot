{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import uuid\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio for running in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize Qdrant clientF\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "#sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "#sentence-transformers/all-mpnet-base-v2\n",
    "MODEL = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load data and create collection\n",
    "df = pd.read_csv('output_embeddings.csv')\n",
    "text_column_name = 'text'\n",
    "\n",
    "# Recreate collection in Qdrant\n",
    "client.recreate_collection(\n",
    "    collection_name=\"similar_text\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Insert data into Qdrant\n",
    "for index, row in df.iterrows():\n",
    "    text = row[text_column_name]\n",
    "    text_embeddings = MODEL.encode(text).tolist()\n",
    "    id = str(uuid.uuid4())\n",
    "    payload = {\"text\": text, \"text_embeddings\": text_embeddings}\n",
    "    client.upsert(\n",
    "        collection_name=\"similar_text\",\n",
    "        wait=True,\n",
    "        points=[PointStruct(id=id, vector=text_embeddings, payload=payload)]\n",
    "    )\n",
    "\n",
    "# Define the input schema for similarity search\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/search\")\n",
    "def search_similar_text(query: Query):\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=3\n",
    "    )\n",
    "    results = [\n",
    "        {\"text\": result.payload[\"text\"], \"similarity_score\": result.score}\n",
    "        for result in search_result\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eb409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6afa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize SentenceTransformer for embeddings\n",
    "MODEL = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Initialize GPT-Neo model\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "\n",
    "# Wrap pipeline with HuggingFacePipeline for LangChain\n",
    "huggingface_llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Define LangChain prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Based on the following context, answer the question concisely without repeating the context: \"\n",
    "        \"Context: {context} \"\n",
    "        \"Question: {question} \"\n",
    "        \"Answer:\"\n",
    "    ),\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=huggingface_llm)\n",
    "\n",
    "# Input schema\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Generate embeddings and search in Qdrant\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=1\n",
    "    )\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"\"\n",
    "\n",
    "    # Generate the answer\n",
    "    raw_answer = llm_chain.run(context=context, question=query.question)\n",
    "\n",
    "    # Trim the output to isolate the relevant text between the first and second occurrence of \"Question\"\n",
    "    first_question_index = raw_answer.find(\"Question\")\n",
    "    second_question_index = raw_answer.find(\"Question\", first_question_index + 1)\n",
    "\n",
    "    if first_question_index != -1 and second_question_index != -1:\n",
    "        clean_answer = raw_answer[first_question_index + len(\"Question:\"):second_question_index].strip()\n",
    "    else:\n",
    "        # Fallback if structure is not as expected\n",
    "        clean_answer = raw_answer.strip()\n",
    "\n",
    "    # Return the response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": clean_answer\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import uuid\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio for running in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize Qdrant clientF\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "#sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "#sentence-transformers/all-mpnet-base-v2\n",
    "MODEL = SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Load data and create collection\n",
    "df = pd.read_csv('output_embeddings.csv')\n",
    "text_column_name = 'text'\n",
    "\n",
    "# Recreate collection in Qdrant\n",
    "client.recreate_collection(\n",
    "    collection_name=\"similar_text\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Insert data into Qdrant\n",
    "for index, row in df.iterrows():\n",
    "    text = row[text_column_name]\n",
    "    text_embeddings = MODEL.encode(text).tolist()\n",
    "    id = str(uuid.uuid4())\n",
    "    payload = {\"text\": text, \"text_embeddings\": text_embeddings}\n",
    "    client.upsert(\n",
    "        collection_name=\"similar_text\",\n",
    "        wait=True,\n",
    "        points=[PointStruct(id=id, vector=text_embeddings, payload=payload)]\n",
    "    )\n",
    "\n",
    "# Define the input schema for similarity search\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/search\")\n",
    "def search_similar_text(query: Query):\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",\n",
    "        query_vector=question_embeddings,\n",
    "        limit=3\n",
    "    )\n",
    "    results = [\n",
    "        {\"text\": result.payload[\"text\"], \"similarity_score\": result.score}\n",
    "        for result in search_result\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers import Qwen2ForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize SentenceTransformer for embeddings\n",
    "MODEL = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Initialize Qwen2 model for text generation\n",
    "model_name = \"Qwen/Qwen2-0.5B-Chat\"  # Update with the appropriate Qwen model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = Qwen2ForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,  # Allows for longer answers\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.3\n",
    ")\n",
    "\n",
    "# Input schema\n",
    "class LLMQuery(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/llm_query\")\n",
    "def llm_query(query: LLMQuery):\n",
    "    # Step 1: Embed the query question\n",
    "    question_embeddings = MODEL.encode(query.question).tolist()\n",
    "\n",
    "    # Step 2: Search for similar context in Qdrant\n",
    "    search_result = client.search(\n",
    "        collection_name=\"similar_text\",  # Replace with your collection name\n",
    "        query_vector=question_embeddings,\n",
    "        limit=3  # Retrieve the top 1 result\n",
    "    )\n",
    "\n",
    "    # Extract context from Qdrant results\n",
    "    context = \" \".join([item.payload[\"text\"] for item in search_result]) if search_result else \"No relevant context found.\"\n",
    "\n",
    "    # Debugging: Log the context and question\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {query.question}\")\n",
    "\n",
    "    # Step 3: Prepare the prompt for Qwen2\n",
    "    prompt = (\n",
    "        f\"Given the following context, provide a short answer to the question.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question:\\n{query.question}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Generate an answer using the Qwen2 model\n",
    "    raw_answer = generator(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Debugging: Log the raw answer\n",
    "    print(f\"Raw Answer: {raw_answer}\")\n",
    "\n",
    "    # Extract the answer from the model output\n",
    "    clean_answer = raw_answer.strip()\n",
    "\n",
    "    # Step 5: Return the response\n",
    "    response = {\n",
    "        \"question\": query.question,\n",
    "        \"context\": context,\n",
    "        \"answer\": clean_answer or \"No answer generated.\"\n",
    "    }\n",
    "    return JSONResponse(content=response, headers={\"Content-Type\": \"application/json\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
